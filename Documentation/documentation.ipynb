{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new.py, named entity detection\n",
    "To detect name entities we check if they are uppper case also use word tagas to see whether they are proper nouns or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk library is used for text processing\n",
    "from nltk import corpus\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words as wn\n",
    "from nltk.corpus import stopwords\n",
    "# Tokenizers divide strings into lists of substrings.  For example,\n",
    "# tokenizers can be used to find the words and punctuation in a string.\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lematizer:\n",
    "# car, cars, car's, cars' --> car\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the file and tokenize it\n",
    "file=open(\"caeser.txt\",'r')\n",
    "text=file.read()\n",
    "sentences=nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "named=set()\n",
    "for sent in sentences:\n",
    "    \n",
    "    if sent.isupper():\n",
    "        named.add(sent)\n",
    "    # pos-tag is process of classifying words into their parts of speech (Noun, adjective etc)\n",
    "    pos=nltk.pos_tag(word_tokenize(sent))\n",
    "    \n",
    "    for i in range(0,len(pos)):\n",
    "#      case that named-entity is two words, check if they have upper case letters\n",
    "        if pos[i][0].isupper() and pos[i+1][0].isupper() and len(pos[i][0])>1 and len(pos[i+1][0])>1 and pos[i+1][1]==\"NNP\":\n",
    "            named.add(pos[i][0]+\" \"+pos[i+1][0])\n",
    "            i=i+1\n",
    "#       if it is one word, check upper case, also check if it is proper noun\n",
    "        elif pos[i][0].isupper() and pos[i][1]==\"NNP\" and len(pos[i][0])>1:\n",
    "            named.add(pos[i][0])\n",
    "        elif pos[i][0][0].isupper() and pos[i][1] == \"NNP\" and len(pos[i][0])>1:\n",
    "            named.add(pos[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "invalidChars = set(string.punctuation.replace(\"_\", \"\"))\n",
    "for word in named:\n",
    "    #print(type(word))\n",
    "    if not any(char in invalidChars for char in word):\n",
    "        if word.isupper():\n",
    "            words.add(word.lower())\n",
    "        elif word[0].isupper():\n",
    "            if not (wordnet_lemmatizer.lemmatize(word.lower()) in wn.words()):\n",
    "                if len(wordnet.synsets(wordnet_lemmatizer.lemmatize(word.lower())))==0:\n",
    "                    words.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in list(words):\n",
    "  thefile.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## slicing.py \n",
    "To discover relations between named-entities, we decided to use graph algorithms and visulisations. Vertices are named-entities and edges represent whether two named-entities named-entities appear together for a given distance to each other. And edge weight is number of occurances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import string\n",
    "import json\n",
    "from nltk import stem,word_tokenize\n",
    "from itertools import product\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary as a word histogram, keys are name entities\n",
    "def words2dict(filename,a_dict):\n",
    "    thefile = open(filename, 'r')\n",
    "    lines=thefile.readlines()\n",
    "    for word in lines:\n",
    "        a_dict[word[:-1]]=a_dict.get(word,0)+1\n",
    "\n",
    "    thefile.close()\n",
    "    return a_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from given text, create a graph,\n",
    "# edges represents two vertices (words)appearing together within range of slice_len\n",
    "# graph dictionary:{(\"daniel,mark\"):1,}\n",
    "# Mike is connected to mark and daniel, they appear n times together\n",
    "\n",
    "def text2graph(filename,slice_len,graph_dict,words_hash):\n",
    "\n",
    "    thefile=open(filename,'r')\n",
    "#   list of punctuations to remove them later\n",
    "    punctuation=list(string.punctuation)\n",
    "\n",
    "    line=thefile.readline()\n",
    "    word_slice=[]\n",
    "    word_count=0\n",
    "    total_count=0\n",
    "    while (line!=\"\"):\n",
    "        total_count+=1\n",
    "#    print progress of loop\n",
    "        if (total_count%1000)==0:\n",
    "            print total_count\n",
    "#       tokenize words\n",
    "        words=word_tokenize(line)\n",
    "        for word in words:\n",
    "            if word in punctuation:\n",
    "                continue\n",
    "            word_count +=1\n",
    "            # lowercase the word\n",
    "            word=word.lower()\n",
    "            # remove affixes\n",
    "            word=stem.WordNetLemmatizer().lemmatize(word)\n",
    "#           if word is one of the name entities\n",
    "            if (words_hash.get(word,None)!=None):\n",
    "                word_slice.append(word)\n",
    "#           if we have enough number of words\n",
    "            if word_count==slice_len:\n",
    "                if len(word_slice)>1:\n",
    "                    \n",
    "#                   create groups of 2 from all name entities in the slice\n",
    "#                   and place them on the graph\n",
    "                    for r in product(word_slice, word_slice):\n",
    "                        r=tuple(sorted(list(r)))\n",
    "                        graph_dict[r]=graph_dict.setdefault(r,0)+1\n",
    "                word_count=0\n",
    "                word_slice=[]\n",
    "        line = thefile.readline().decode('utf-8')\n",
    "\n",
    "    thefile.close()\n",
    "\n",
    "    return graph_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file with name entities\n",
    "words_filename=\"macbeth-named.txt\"\n",
    "# entire text\n",
    "text_filename=\"macbeth.txt\"\n",
    "# slice length parameter\n",
    "slice_len=10\n",
    "# graph and name_entity histogram dictionaries\n",
    "graph_dict={}\n",
    "words_hash={}\n",
    "words_hash=words2dict(words_filename,words_hash)\n",
    "graph_dict=text2graph(text_filename,slice_len,graph_dict,words_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Changing data structure of graph for data processing and visulisation\n",
    "\n",
    "# list of name entities \n",
    "names=[]\n",
    "names_dict={}\n",
    "for item in graph_dict.keys():\n",
    "    names.append(item[0])\n",
    "    names.append(item[1])\n",
    "\n",
    "# generate keys for each name entity\n",
    "names=list(set(names))\n",
    "idx=[]\n",
    "for i,k in enumerate(names):\n",
    "    idx.append((i,k))\n",
    "    names_dict[k]=i\n",
    "\n",
    "# save them as csv files\n",
    "ofile  = open('id_name.csv', \"wb\")\n",
    "for (i,item) in idx:\n",
    "    ofile.write(\"%d,%s\"%(i,item))\n",
    "    ofile.write(\"\\n\")\n",
    "ofile.close()\n",
    "\n",
    "ofile  = open('edges.csv', \"wb\")\n",
    "\n",
    "for item in graph_dict.items():\n",
    "    ofile.write(\"%d,%d,%d\"%(names_dict[item[0][0]],names_dict[item[0][1]],item[1]))\n",
    "    ofile.write(\"\\n\")\n",
    "\n",
    "ofile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing2.py \n",
    "When we pick our text source as Shakespeare plays, we decided to use a better heuristic than slice lenght. Because they have dialogs, we decided to use length of a dialog as our slice length. So this new slicing script gets text, and generates graphs from this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "\n",
    "file=open(\"caeser.txt\")\n",
    "text=file.read()\n",
    "\n",
    "with open(\"caeser-named.txt\") as f:\n",
    "    text1=f.readlines()\n",
    "    \n",
    "names = [x.strip() for x in text1]\n",
    "names_dict=[]\n",
    "g={}\n",
    "count=0\n",
    "acts=text.split('\\n\\n')\n",
    "\n",
    "for name in names:\n",
    "    g={}\n",
    "    g[\"id\"]=count\n",
    "    g[\"label\"]=name\n",
    "    names_dict.append(g)\n",
    "    count=count+1\n",
    "edges_dict=[]\n",
    "ed={}\n",
    "\n",
    "l=list(itertools.combinations(range(len(names_dict)), 2))\n",
    "for pair in l:\n",
    "    ed={}\n",
    "    ed[\"source\"]=pair[0]\n",
    "    ed[\"target\"]=pair[1]\n",
    "    ed[\"weight\"]=0\n",
    "    edges_dict.append(ed)\n",
    "\n",
    "for pair in edges_dict:\n",
    "    for act in acts:\n",
    "        s1=act.lower().count(names_dict[pair[\"source\"]][\"label\"])\n",
    "        s2=act.lower().count(names_dict[pair[\"target\"]][\"label\"])\n",
    "        pair[\"weight\"]=pair[\"weight\"]+min(s1,s2)\n",
    "        \n",
    "# save edges that are \n",
    "final_edge=[]\n",
    "for edge in edges_dict:\n",
    "    #print(edge)\n",
    "    if edge['weight']!=0:\n",
    "        final_edge.append(edge)\n",
    "# print(final_edge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save results\n",
    "keys = names_dict[0].keys()\n",
    "with open('id_name1.csv', 'w',newline='') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(names_dict)\n",
    "\n",
    "keys = final_edge[0].keys()\n",
    "with open('edges1.csv', 'w',newline='') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(final_edge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
